{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bdd31f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import errno\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import deepcell\n",
    "from deepcell.utils.tracking_utils import load_trks, trks_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bde8706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e45ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 15714735188428714538]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "devices = device_lib.list_local_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd88347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5722ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the data file is currently required for `train_model_()` functions\n",
    "\n",
    "# Change DATA_DIR if you are not using `deepcell.datasets`\n",
    "\n",
    "# DATA_FILE should be a trks file (contains 2 np arrays and a lineage dictionary)\n",
    "##DATA_FILE = \"../trks_141923_repeat.trks\"\n",
    "DATA_FILE = \"../train.npz\"\n",
    "TEST_FILE = '../test.npz'  ## Added\n",
    "\n",
    "DATA_DIR = '~'\n",
    "# confirm the data file is available\n",
    "assert os.path.isfile(DATA_FILE)\n",
    "#assert os.path.isfile(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89bdcb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: \n",
      "Image data shape:  (440, 100, 530, 530, 1)\n",
      "Number of lineages (should equal batch size):  440\n",
      "Total number of unique tracks (cells)      -  988\n",
      "Total number of divisions                  -  0\n",
      "Average cell density (cells/100 sq pixels) -  0.0004974271012006861\n",
      "Average number of frames per track         -  75\n",
      "Dataset Statistics: \n",
      "Image data shape:  (66, 100, 530, 530, 1)\n",
      "Number of lineages (should equal batch size):  66\n",
      "Total number of unique tracks (cells)      -  174\n",
      "Total number of divisions                  -  0\n",
      "Average cell density (cells/100 sq pixels) -  0.0004903071296805722\n",
      "Average number of frames per track         -  64\n"
     ]
    }
   ],
   "source": [
    "trks_stats('../train.trks')\n",
    "trks_stats('../test.trks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75bf6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up other required filepaths\n",
    "\n",
    "# If the data file is in a subdirectory, mirror it in MODEL_DIR and LOG_DIR\n",
    "PREFIX = 'test_0814'\n",
    "\n",
    "ROOT_DIR = '../train_0814'  # TODO: Change this! Usually a mounted volume\n",
    "MODEL_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'models', PREFIX))\n",
    "LOG_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'logs', PREFIX))\n",
    "\n",
    "# create directories if they do not exist\n",
    "for d in (MODEL_DIR, LOG_DIR):\n",
    "    try:\n",
    "        os.makedirs(d)\n",
    "    except OSError as exc:  # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b59c6e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "n_epoch = 10    # Number of training epochs\n",
    "test_size = .1  # % of data saved as validation\n",
    "train_seed = 1   # Random seed for training/validation data split\n",
    "\n",
    "optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "lr_sched = rate_scheduler(lr=0.01, decay=0.99)\n",
    "\n",
    "# Tracking training settings\n",
    "features = {'appearance', 'distance', 'neighborhood', 'regionprop'}\n",
    "min_track_length = 9\n",
    "neighborhood_scale_size = 30\n",
    "batch_size = 16  # changed  \n",
    "crop_dim = 100  # changed\n",
    "in_shape = (crop_dim, crop_dim, 1)\n",
    "\n",
    "model_name = 'tracking_model_seed{}_tl{}'.format(train_seed, min_track_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec3eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22531fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepcell.image_generators as generators\n",
    "from deepcell.utils.data_utils import get_data\n",
    "\n",
    "# Get the data\n",
    "#train_dict, test_dict = get_data(DATA_FILE, mode='siamese_daughters',\n",
    "#                                 seed=train_seed, test_size=test_size)\n",
    "train_dict, test_dict = get_data(DATA_FILE, mode='sample', seed=train_seed, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fa638d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(396, 100, 530, 530, 1)\n",
      "(44, 100, 530, 530, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_dict['X'].shape)\n",
    "print(test_dict['X'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc72fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the generators and iterators\n",
    "datagen_train = generators.SiameseDataGenerator(\n",
    "    rotation_range=0, # randomly rotate images by 0 to rotation_range degrees\n",
    "    shear_range=0,      # randomly shear images in the range (radians , -shear_range to shear_range)\n",
    "    horizontal_flip=0,  # randomly flip images\n",
    "    vertical_flip=0)    # randomly flip images\n",
    "\n",
    "train_data = datagen_train.flow(\n",
    "    #test_dict,  # original\n",
    "    train_dict,\n",
    "    batch_size=batch_size,\n",
    "    seed=train_seed,\n",
    "    crop_dim=crop_dim,\n",
    "    neighborhood_scale_size=neighborhood_scale_size,\n",
    "    min_track_length=min_track_length,\n",
    "    features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0d55c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_test = generators.SiameseDataGenerator(\n",
    "    rotation_range=0,  # randomly rotate images by 0 to rotation_range degrees\n",
    "    shear_range=0,     # randomly shear images in the range (radians , -shear_range to shear_range)\n",
    "    horizontal_flip=0, # randomly flip images\n",
    "    vertical_flip=0)   # randomly flip images\n",
    "\n",
    "test_data = datagen_test.flow(\n",
    "    test_dict,\n",
    "    batch_size=batch_size,\n",
    "    seed=train_seed,\n",
    "    crop_dim=crop_dim,\n",
    "    neighborhood_scale_size=neighborhood_scale_size,\n",
    "    min_track_length=min_track_length,\n",
    "    features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb780f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "204e624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell import model_zoo\n",
    "\n",
    "tracking_model = model_zoo.siamese_model(\n",
    "    input_shape=in_shape,\n",
    "    neighborhood_scale_size=neighborhood_scale_size,\n",
    "    features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "729c5250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell import losses\n",
    "\n",
    "n_classes = tracking_model.layers[-1].output_shape[-1]\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    return losses.weighted_categorical_crossentropy(y_true, y_pred,\n",
    "                                                    n_classes=n_classes,\n",
    "                                                    from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf23da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa785418",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_model.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b93280",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Call `fit_generator` on the compiled model, along with a default set of callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "877d5fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch= train_dict['y'].shape[0] // batch_size\n",
    "b = np.array(steps_per_epoch).astype(np.float32)\n",
    "print(b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b119e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 0 GPUs.\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - 282s 12s/step - loss: 0.1128 - accuracy: 0.9375 - val_loss: 0.2901 - val_accuracy: 0.7188\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.29009, saving model to /home/jupyter/train_0814/models/test_0814/tracking_model_seed1_tl9.h5\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 272s 11s/step - loss: 0.1041 - accuracy: 0.9378 - val_loss: 0.3838 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.29009\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 282s 12s/step - loss: 0.0860 - accuracy: 0.9661 - val_loss: 0.0865 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.29009 to 0.08646, saving model to /home/jupyter/train_0814/models/test_0814/tracking_model_seed1_tl9.h5\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 282s 12s/step - loss: 0.0828 - accuracy: 0.9688 - val_loss: 0.0579 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08646 to 0.05790, saving model to /home/jupyter/train_0814/models/test_0814/tracking_model_seed1_tl9.h5\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 272s 11s/step - loss: 0.0959 - accuracy: 0.9514 - val_loss: 0.0814 - val_accuracy: 0.9688\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.05790\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 272s 11s/step - loss: 0.0757 - accuracy: 0.9730 - val_loss: 0.0347 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.05790 to 0.03474, saving model to /home/jupyter/train_0814/models/test_0814/tracking_model_seed1_tl9.h5\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 282s 12s/step - loss: 0.0944 - accuracy: 0.9609 - val_loss: 0.0649 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.03474\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 282s 12s/step - loss: 0.0764 - accuracy: 0.9661 - val_loss: 0.0895 - val_accuracy: 0.9688\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.03474\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 282s 12s/step - loss: 0.0815 - accuracy: 0.9583 - val_loss: 0.0480 - val_accuracy: 0.9688\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.03474\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 271s 11s/step - loss: 0.0591 - accuracy: 0.9784 - val_loss: 0.0475 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.03474\n"
     ]
    }
   ],
   "source": [
    "from deepcell.utils.train_utils import get_callbacks\n",
    "from deepcell.utils.train_utils import count_gpus\n",
    "from deepcell.utils import tracking_utils\n",
    "\n",
    "\n",
    "model_path = os.path.join(MODEL_DIR, '{}.h5'.format(model_name))\n",
    "loss_path = os.path.join(MODEL_DIR, '{}.npz'.format(model_name))\n",
    "\n",
    "num_gpus = count_gpus()\n",
    "\n",
    "print('Training on', num_gpus, 'GPUs.')\n",
    "\n",
    "train_callbacks = get_callbacks(\n",
    "    model_path,\n",
    "    lr_sched=lr_sched,\n",
    "    tensorboard_log_dir=LOG_DIR,\n",
    "    save_weights_only=num_gpus >= 2,\n",
    "    monitor='val_loss',\n",
    "    verbose=1)\n",
    "\n",
    "# rough estimate for steps_per_epoch\n",
    "#total_train_pairs = tracking_utils.count_pairs(train_dict['y'], same_probability=5.0)\n",
    "#total_test_pairs = tracking_utils.count_pairs(test_dict['y'], same_probability=5.0)\n",
    "steps_per_epoch= train_dict['y'].shape[0] // batch_size\n",
    "validation_steps = test_dict['y'].shape[0] // batch_size\n",
    "\n",
    "#print(steps_per_epoch.dtype)\n",
    "\n",
    "# fit the model on the batches generated by datagen.flow()\n",
    "loss_history = tracking_model.fit_generator(\n",
    "    train_data,\n",
    "    #steps_per_epoch=total_train_pairs // batch_size,\n",
    "    steps_per_epoch=np.array(steps_per_epoch).astype(np.float32),\n",
    "    validation_data=test_data,\n",
    "    #validation_steps=total_test_pairs // batch_size,\n",
    "    epochs=10,\n",
    "    validation_steps = np.array(validation_steps).astype(np.float32),\n",
    "    callbacks=train_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "528a17ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dict['y'])//batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee63eed",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "\n",
    "**Requires a Seed Value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe8a137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rosary added\n",
    "## Redefine test_data\n",
    "\n",
    "train_dict, test_dict = get_data(TEST_FILE, mode='sample', seed=train_seed, test_size=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d02b905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datagen_test.flow(\n",
    "    test_dict,\n",
    "    batch_size=batch_size,\n",
    "    seed=train_seed,\n",
    "    crop_dim=crop_dim,\n",
    "    neighborhood_scale_size=neighborhood_scale_size,\n",
    "    min_track_length=min_track_length,\n",
    "    features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3db00d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.2238530e-02, 9.7749251e-01, 2.6892775e-04],\n",
       "       [6.5876590e-03, 9.8986036e-01, 3.5520762e-03],\n",
       "       [4.0780385e-03, 9.9536538e-01, 5.5665779e-04],\n",
       "       [9.4052845e-01, 5.7981577e-02, 1.4899881e-03],\n",
       "       [1.1098699e-01, 8.8694257e-01, 2.0703997e-03],\n",
       "       [7.9518398e-03, 9.9151236e-01, 5.3578313e-04],\n",
       "       [1.4865002e-02, 9.8447084e-01, 6.6412718e-04],\n",
       "       [2.8137362e-02, 9.7079873e-01, 1.0639080e-03],\n",
       "       [6.1087590e-03, 9.9196047e-01, 1.9307449e-03],\n",
       "       [1.2552477e-02, 9.8624194e-01, 1.2055965e-03],\n",
       "       [1.5114242e-02, 9.8393548e-01, 9.5031550e-04],\n",
       "       [1.4350423e-01, 8.5544914e-01, 1.0466333e-03],\n",
       "       [8.5899487e-02, 9.1268992e-01, 1.4105946e-03],\n",
       "       [2.3103507e-02, 9.7575164e-01, 1.1448293e-03],\n",
       "       [5.9303150e-02, 9.3952590e-01, 1.1709327e-03],\n",
       "       [1.0987557e-02, 9.8722410e-01, 1.7883488e-03]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst, y_true = next(test_data)\n",
    "tracking_model.predict(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a578c0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........\n",
      "[[4827  437]\n",
      " [   0 9095]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y = []\n",
    "Y_pred = []\n",
    "\n",
    "for i in range(1,1000):\n",
    "    if i % 100 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "    lst, y_true = next(test_data)\n",
    "    y_true = np.argmax(y_true['classification'], axis=-1)\n",
    "    y_pred = np.argmax(tracking_model.predict(lst), axis=-1)\n",
    "    Y.append(y_true)\n",
    "    Y_pred.append(y_pred)\n",
    "    \n",
    "Y = np.concatenate(Y, axis=0)\n",
    "Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "\n",
    "print(\"\")\n",
    "cm = confusion_matrix(Y, Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b195052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy across all three classes:  0.9695661257747754\n",
      "Accuracy for each individual class [Different, Same, Daughter]:  [0.91698328 1.        ]\n"
     ]
    }
   ],
   "source": [
    "test_acc = sum(np.array(Y) == np.array(Y_pred)) / len(Y)\n",
    "print('Accuracy across all three classes: ', test_acc)\n",
    "\n",
    "# Normalize the diagonal entries of the confusion matrix\n",
    "cm = cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]\n",
    "# Diagonal entries are the accuracies of each class\n",
    "print('Accuracy for each individual class [Different, Same, Daughter]: ', cm.diagonal())"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m76"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
