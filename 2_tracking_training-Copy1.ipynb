{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141c6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import deepcell\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import deepcell\n",
    "from deepcell.utils.tracking_utils import load_trks, trks_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ae075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.utils.data_utils import get_data\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "def load_data(path, mode, test_size=0.1, seed=0):\n",
    "    \"\"\"Loads dataset.\n",
    "    Args:\n",
    "         test_size (float): fraction of data to reserve as test data\n",
    "        seed (int): the seed for randomly shuffling the dataset\n",
    "    Returns:\n",
    "           tuple: (x_train, y_train), (x_test, y_test).\n",
    "    \"\"\"\n",
    "\n",
    "    train_dict, test_dict = get_data(\n",
    "        path,\n",
    "        mode=mode,\n",
    "        test_size=test_size,\n",
    "        seed=seed)\n",
    "    x_train, y_train = train_dict['X'], train_dict['y']\n",
    "    x_test, y_test = test_dict['X'], test_dict['y']\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf406907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66cc5db0",
   "metadata": {},
   "source": [
    "## Set up file path constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c9e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "\n",
    "ROOT_DIR = '../track_train/attempt_2'  # TODO: Change this! Usually a mounted volume\n",
    "\n",
    "MODEL_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'models'))\n",
    "LOG_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'logs'))\n",
    "#DATA_DIR = os.path.expanduser(os.path.join('~', '.keras', 'datasets'))\n",
    "OUTPUT_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'nuc_tracking'))\n",
    "\n",
    "# create directories if they do not exist\n",
    "for d in (MODEL_DIR, LOG_DIR, OUTPUT_DIR):\n",
    "    try:\n",
    "        os.makedirs(d)\n",
    "    except OSError as exc:  # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19c7e84d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20206/3252988249.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../train.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sample'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## Try siamese mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X.shape: {}\\ny.shape: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20206/3025294212.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path, mode, test_size, seed)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         seed=seed)\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/DeepCell-0.10.0rc2-py3.7.egg/deepcell/utils/data_utils.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(file_name, mode, test_size, seed)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 return format.read_array(bytes,\n\u001b[1;32m    254\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[1;32m    256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    765\u001b[0m                                                              count=read_count)\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_crc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0;31m# No need to compute the CRC if we don't have a reference value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m         \u001b[0;31m# Check the CRC if we're at the end of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expected_crc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data('../train.npz', mode='sample')  ## Try siamese mode\n",
    "print('X.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trks_stats(\"../train.trks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f3db03",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "To facilitate training, we transform each movie's image and lineage data into a `Track` object.\n",
    "`Tracks` help to encapsulate all of the feature creation from the movie, including:\n",
    "\n",
    "* Appearances: `(num_frames, num_objects, 32, 32, 1)`\n",
    "* Morphologies: `(num_frames, num_objects, 32, 32, 3)`\n",
    "* Centroids: `(num_frames, num_objects, 2)`\n",
    "* Normalized Adjacency Matrix: `(num_frames, num_objects, num_objects, 3)`\n",
    "* Temporal Adjacency Matrix (comparing across frames): `(num_frames - 1, num_objects, num_objects, 3)`\n",
    "\n",
    "All `Track` objects are then concatenated with each other (using `concat_tracks`) resulting in a single array for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ea6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use 22956814_seq.trks as a test first  ### Because it requires sequential labels as well\n",
    "\n",
    "trks_data = load_trks('../train.trks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d0499",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trks_data)  # Is it a list??? Or an object... ## Better to be a list! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trks_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trks_data['lineages'])  ### GGGRRRRRREAT!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf08a38",
   "metadata": {},
   "source": [
    "### Modify the lineages data -> into strings!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6841e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lineages = trks_data['lineages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lineages_list_new = []\n",
    "\n",
    "for index in range(len(lineages)):    \n",
    "    lineages_new = {}\n",
    "\n",
    "    for i in range(len(list(lineages[index].keys()))):\n",
    "        #lineages_new[str(list(lineages[index].keys())[i])] = lineages[index][list(lineages[index].keys())[i]]\n",
    "        \n",
    "        worm = str(list(lineages[index].keys())[i])\n",
    "        \n",
    "        #lineages_new[str(list(lineages[index].keys())[i])] = {}\n",
    "        #lineages_new[str(list(lineages[index].keys())[i])]['label'] = str(lineages[index][list(lineages[index].keys())[i]]['label'])\n",
    "        \n",
    "        lineages_new[worm] = {}\n",
    "        lineages_new[worm]['label'] = str(lineages[index][list(lineages[index].keys())[i]]['label'])\n",
    "        lineages_new[worm]['frames'] = [str(frame) for frame in lineages[index][list(lineages[index].keys())[i]]['frames']]\n",
    "        lineages_new[worm]['parent'] = None\n",
    "        lineages_new[worm]['daughters'] = []\n",
    "        #print(lineages_new)\n",
    "    lineages_list_new.append(lineages_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lineages_list_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaae5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trks_data['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ba6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trks_data_modified = {}\n",
    "trks_data_modified['X'] = trks_data['X']\n",
    "trks_data_modified['y'] = trks_data['y']\n",
    "trks_data_modified['lineages'] = lineages_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lineage[cell_id]['parent']\n",
    "print(trks_data_modified['lineages'][0]['9']['parent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0527cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from deepcell_tracking.utils import Track\n",
    "\n",
    "all_tracks = Track(tracked_data=trks_data_modified, appearance_dim=40, distance_threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e427f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from deepcell_tracking.utils import concat_tracks\n",
    "\n",
    "track_info = concat_tracks([all_tracks])\n",
    "for k, v in track_info.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d86cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "######GRRRRRRRRRRREAT!!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390f7e6",
   "metadata": {},
   "source": [
    "## Set up training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529c655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1   # random seed for training/validation data split\n",
    "batch_size = 16\n",
    "track_length = 20  # only train on 8 frames at once  ## 8\n",
    "val_size = .20  # % of data saved as validation\n",
    "test_size = .1  # % of data held out as a test set\n",
    "n_epochs = 12  # number of training epochs\n",
    "\n",
    "steps_per_epoch = 50  ## 1000\n",
    "validation_steps = 10  ## 200\n",
    "\n",
    "# find maximum number of cells in any frame\n",
    "max_cells = track_info['appearances'].shape[2]\n",
    "\n",
    "n_layers = 1  # number of graph convolutions\n",
    "\n",
    "translation_range = X_train.shape[-2]\n",
    "\n",
    "model_name = 'graph_tracking_model_seed{}'.format(seed)\n",
    "model_path = os.path.join(MODEL_DIR, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae2d2f",
   "metadata": {},
   "source": [
    "## Create Tracking Dataset object\n",
    "\n",
    "We then assemble `Tracks` as specified into a Tracking `Dataset` that is used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e71f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.data.tracking import prepare_dataset\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = prepare_dataset(\n",
    "    track_info,\n",
    "    rotation_range=180,\n",
    "    translation_range=translation_range,\n",
    "    seed=seed,\n",
    "    val_size=val_size,\n",
    "    test_size=test_size,\n",
    "    batch_size=batch_size,\n",
    "    track_length=track_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e28c6",
   "metadata": {},
   "source": [
    "## Instantiate and Compile the Model\n",
    "The goal is to predict the adjacency matrix and daughter adjacency matrix by attending over the edges provided by the spatiotemporal adjacency matrix (SAM). The SAM is constructed by linking cells that are in temporal or spatial proximity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f3640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.model_zoo.tracking import GNNTrackingModel\n",
    "\n",
    "tm = GNNTrackingModel(max_cells=max_cells, n_layers=n_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0431f2",
   "metadata": {},
   "source": [
    "Before a model can be trained, it must be compiled with the chosen optimizer, loss function, and metrics.\n",
    "\n",
    "This model uses padded data which must be flattened and filtered out before calling our metrics and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a4e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_flatten(y_true, y_pred):\n",
    "    n_classes = tf.shape(y_true)[-1]\n",
    "    new_shape = [-1, n_classes]\n",
    "    y_true = tf.reshape(y_true, new_shape)\n",
    "    y_pred = tf.reshape(y_pred, new_shape)\n",
    "\n",
    "    # Mask out the padded cells\n",
    "    good_loc = tf.where(y_true[:, 0] != -1)[:, 0]\n",
    "\n",
    "    y_true = tf.gather(y_true, good_loc, axis=0)\n",
    "    y_pred = tf.gather(y_pred, good_loc, axis=0)\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "class Recall(tf.keras.metrics.Recall):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "        super(Recall, self).update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "class Precision(tf.keras.metrics.Precision):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "        super(Precision, self).update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "    return deepcell.losses.weighted_categorical_crossentropy(\n",
    "        y_true, y_pred,\n",
    "        n_classes=tf.shape(y_true)[-1],\n",
    "        axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05262578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.optimizers import RectifiedAdam as RAdam\n",
    "\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = RAdam(lr=1e-3, clipnorm=0.001)\n",
    "\n",
    "# Define the loss function\n",
    "losses = {'temporal_adj_matrices': loss_function}\n",
    "\n",
    "# Define metrics\n",
    "metrics = [\n",
    "    Recall(class_id=0, name='same_recall'),\n",
    "    Recall(class_id=1, name='different_recall'),\n",
    "    Recall(class_id=2, name='daughter_recall'),\n",
    "    Precision(class_id=0, name='same_precision'),\n",
    "    Precision(class_id=1, name='different_precision'),\n",
    "    Precision(class_id=2, name='daughter_precision'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d0abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "tm.training_model.compile(optimizer=optimizer, loss=losses, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448b919d",
   "metadata": {},
   "source": [
    "## Train Model and Verify Performance\n",
    "\n",
    "Call fit on the compiled model, along with a defined set of callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f3fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "# Verify GPU count\n",
    "from deepcell import train_utils\n",
    "num_gpus = train_utils.count_gpus()\n",
    "print('Training on {} GPUs'.format(num_gpus))\n",
    "\n",
    "# Train the model\n",
    "train_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, monitor='val_loss',\n",
    "        save_best_only=True, verbose=1,\n",
    "        save_weights_only=False),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, verbose=1,\n",
    "        patience=3, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "loss_history = tm.training_model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=n_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=train_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f1a012",
   "metadata": {},
   "source": [
    "Visualize the performance of the model by comparing the predicted classes of i frames in j movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770c10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "it = val_data.as_numpy_iterator()\n",
    "x_temp, y_temp = it.next()\n",
    "\n",
    "temp_adj = y_temp['temporal_adj_matrices']\n",
    "\n",
    "output = tm.training_model.predict(x_temp)\n",
    "\n",
    "fig, axes = plt.subplots(8, 7, figsize=(80, 80))\n",
    "\n",
    "for j in range(4):\n",
    "    for i in range(7):\n",
    "        pred = output[j, i, ...]\n",
    "        truth = temp_adj[j, i, ...]\n",
    "\n",
    "        summed = np.sum(truth, axis=-1)\n",
    "        bad_loc = np.where(summed == -3)[1]\n",
    "        end = bad_loc[0] - 1\n",
    "        summed = np.sum(truth, axis=-1)\n",
    "\n",
    "        axes[2 * j, i].imshow(\n",
    "            np.argmax(pred[0:end, 0:end, :], axis=-1),\n",
    "            cmap='jet', vmin=0, vmax=2)\n",
    "        axes[2 * j + 1, i].imshow(\n",
    "            np.argmax(truth[0:end, 0:end, :]==1, axis=-1),\n",
    "            cmap='jet', vmin=0, vmax=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0608f411",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa47050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models for prediction\n",
    "inf_path = os.path.join(MODEL_DIR, 'TrackingModelInf')\n",
    "ne_path = os.path.join(MODEL_DIR, 'TrackingModelNE')\n",
    "\n",
    "#tm.inference_model.save(inf_path)\n",
    "#tm.neighborhood_encoder.save(ne_path)\n",
    "\n",
    "## Use save_weights\n",
    "tm.inference_model.save_weights(inf_path)\n",
    "tm.neighborhood_encoder.save_weights(ne_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8413f66",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This model is used within an assignment problem framework to track cells through time-lapse sequences and build cell lineages. To see how this works on example data, see below. \n",
    "\n",
    "To use existing models for tracking (and segmentation), refer to its counterpart in the `deepcell.application` [notebook series](https://github.com/vanvalenlab/deepcell-tf/blob/master/notebooks/applications/Nuclear-Application.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a31e263",
   "metadata": {},
   "source": [
    "## Load the Model and Track One Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bdf5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Models\n",
    "from deepcell.model_zoo.tracking import GNNTrackingModel\n",
    "tm = GNNTrackingModel()\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Load models for prediction\n",
    "# (update `ne_path` and `inf_path` if different from above)\n",
    "##tm.neighborhood_encoder = tf.keras.models.load_model(ne_path)\n",
    "##tm.inference_model = tf.keras.models.load_model(inf_path)\n",
    "\n",
    "tm.neighborhood_encoder.load_weights(ne_path)\n",
    "tm.inference_model.load_weights(inf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686ca255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alter below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeedd9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose and load a dataset to track using the trained model\n",
    "benchmark_filename = '../22956814/22956814_str.trks'  ## Use a deepcell segmented? ### TODO: later\n",
    "(X_train, y_train), (X_test, y_test) = load_data(\"../22956814/22956814_str.npz\", mode='sample')\n",
    "print('Val Data -\\nX.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))\n",
    "\n",
    "path_bench_trks = os.path.join(benchmark_filename)\n",
    "test_data = load_trks(path_bench_trks)\n",
    "\n",
    "benchmark_index = 1  # this one has divisions\n",
    "\n",
    "raw_images = test_data['X'][benchmark_index]\n",
    "labeled_movie = test_data['y'][benchmark_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f73804",
   "metadata": {},
   "source": [
    "### Import the cell tracking algorithm and track the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3273997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell_tracking import CellTracker\n",
    "\n",
    "npz = np.load(\"../6814_2/6814_2_str.npz\")\n",
    "raw_images = npz['X'][0]\n",
    "labeled_movie = npz['y'][0]\n",
    "\n",
    "\n",
    "\n",
    "cell_tracker = CellTracker(\n",
    "    movie=raw_images,\n",
    "    annotation=labeled_movie,\n",
    "    track_length=track_length,\n",
    "    neighborhood_encoder=tm.neighborhood_encoder,\n",
    "    tracking_model=tm.inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d9c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cell_tracker.track_cells()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f14e4b",
   "metadata": {},
   "source": [
    "### Review the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c78331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View tracked results of each batch as a video\n",
    "# NB: This does not render well on GitHub\n",
    "from IPython.display import HTML\n",
    "from deepcell.utils.plot_utils import get_js_video\n",
    "\n",
    "# Raw\n",
    "HTML(get_js_video(np.expand_dims(raw_images, axis=0),\n",
    "                  batch=0, cmap='gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracked\n",
    "\n",
    "# Scale the colors to match the max cell label\n",
    "HTML(get_js_video(np.expand_dims(cell_tracker.y_tracked, axis=0),\n",
    "                  batch=0, cmap='cubehelix', vmin=0,\n",
    "                  vmax=len(cell_tracker.tracks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a801d6b3",
   "metadata": {},
   "source": [
    "# Save the Output\n",
    "\n",
    "If desired, save the results in our compressed format (.trk - with lineage information), as a movie (.gif - images only/no lineage information), or both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83c01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trk file\n",
    "res_file_path = os.path.join(OUTPUT_DIR, 'hek_test.trk')\n",
    "cell_tracker.dump(res_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c8214e",
   "metadata": {},
   "source": [
    "Optionally, save the output as `.tif` files or `.gif` files for easy inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "vmax = len(cell_tracker.y_tracked)\n",
    "\n",
    "raw = []\n",
    "tracked = []\n",
    "\n",
    "for i in range(cell_tracker.X.shape[0]):\n",
    "    new_image = cell_tracker.X[i, ..., 0]\n",
    "    raw_path = os.path.join(OUTPUT_DIR, 'image_%d.tiff' % i)\n",
    "    imageio.imwrite(raw_path, new_image.astype('uint8'))\n",
    "    raw.append(new_image.astype('uint8'))\n",
    "\n",
    "    label_image = cell_tracker.y_tracked[i, ..., 0].astype('uint8')\n",
    "    label_path = os.path.join(OUTPUT_DIR, 'label_%d.tiff' % i)\n",
    "    plt.imsave(label_path, label_image,\n",
    "               cmap='cubehelix', vmin=0, vmax=vmax)\n",
    "    # imageio.imwrite(label_path, label_image,\n",
    "    #                 cmap='cubehelix', vmin=0, vmax=vmax)\n",
    "    tracked.append(label_image.astype('uint8'))\n",
    "\n",
    "# Make gifs\n",
    "imageio.mimsave(os.path.join(OUTPUT_DIR, 'raw.gif'), raw)\n",
    "imageio.mimsave(os.path.join(OUTPUT_DIR, 'tracked.gif'), tracked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0edd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarked results not included here... See the original notebook"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m78"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
