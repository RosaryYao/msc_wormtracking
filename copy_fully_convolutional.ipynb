{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Convolutional Interior/Edge Segmentation for 2D Data\n",
    "\n",
    "---\n",
    "\n",
    "Classifies each pixel as either Cell Edge, Cell Interior, or Background.\n",
    "\n",
    "There are 2 different Cell Edge classes (Cell-Cell Boundary and Cell-Background Boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import deepcell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.utils.data_utils import get_data\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "def load_data(path, mode, test_size=0.2, seed=0):\n",
    "    \"\"\"Loads dataset.\n",
    "    Args:\n",
    "         test_size (float): fraction of data to reserve as test data\n",
    "        seed (int): the seed for randomly shuffling the dataset\n",
    "    Returns:\n",
    "           tuple: (x_train, y_train), (x_test, y_test).\n",
    "    \"\"\"\n",
    "    #basepath = os.path.expanduser(os.path.join('~', '.keras', 'datasets'))\n",
    "    #prefix = path.split(os.path.sep)[:-1]\n",
    "    #data_dir = os.path.join(basepath, *prefix) if prefix else basepath\n",
    "    #if not os.path.exists(data_dir):\n",
    "    #    os.makedirs(data_dir)\n",
    "    #elif not os.path.isdir(data_dir):\n",
    "    #    raise IOError('{} exists but is not a directory'.format(data_dir))\n",
    "\n",
    "    train_dict, test_dict = get_data(\n",
    "        path,\n",
    "        mode=mode,\n",
    "        test_size=test_size,\n",
    "        seed=seed)\n",
    "    x_train, y_train = train_dict['X'], train_dict['y']\n",
    "    x_test, y_test = test_dict['X'], test_dict['y']\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "### Download the data from `deepcell.datasets`\n",
    "\n",
    "`deepcell.datasets` provides access to a set of annotated live-cell imaging datasets which can be used for training cell segmentation and tracking models.\n",
    "All dataset objects share the `load_data()` method, which allows the user to specify the name of the file (`path`), the fraction of data reserved for testing (`test_size`) and a `seed` which is used to generate the random train-test split.\n",
    "Metadata associated with the dataset can be accessed through the `metadata` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data (saves to ~/.keras/datasets)\n",
    "#filename = 'HeLa_S3.npz'\n",
    "#test_size = 0.2 # % of data saved as test\n",
    "#seed = 0 # seed for random train-test split\n",
    "\n",
    "#(X_train, y_train), (X_test, y_test) = deepcell.datasets.hela_s3.load_data(filename, test_size=test_size, seed=seed)\n",
    "\n",
    "#print('X.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#file = np.load('../train.npz')\n",
    "#file['X'].shape\n",
    "#X = np.vstack(file['X'])\n",
    "#X = X[0:-1:250]\n",
    "#print(X.shape)\n",
    "\n",
    "#y = np.vstack(file['y'])[0:-1:250]\n",
    "#print(y.shape)\n",
    "\n",
    "#np.savez(\"../seg_train.npz\", X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (140, 530, 530, 1)\n",
      "y.shape: (140, 530, 530, 1)\n"
     ]
    }
   ],
   "source": [
    "#data = np.load('../seg_train.npz')\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data('../seg_train.npz', mode='sample')\n",
    "print('X.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up filepath constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up other required filepaths\n",
    "\n",
    "PREFIX = 'test'\n",
    "\n",
    "ROOT_DIR = '../seg_training/attempt_2'  \n",
    "MODEL_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'models', PREFIX))\n",
    "LOG_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'logs', PREFIX))\n",
    "\n",
    "# create directories if they do not exist\n",
    "for d in (MODEL_DIR, LOG_DIR):\n",
    "    try:\n",
    "        os.makedirs(d)\n",
    "    except OSError as exc:  # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Foreground/Background FeatureNet Model\n",
    "\n",
    "Here we instantiate two `FeatureNet` models from `deepcell.model_zoo` for foreground/background separation as well as the interior/edge segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_method = 'std'  # data normalization\n",
    "receptive_field = 61  # should be adjusted for the scale of the data  ## 61\n",
    "n_skips = 1  # number of skip-connections (only for FC training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixelwise transform settings\n",
    "dilation_radius = 1  # change dilation radius for edge dilation\n",
    "separate_edge_classes = True  # break edges into cell-background edge, cell-cell edge\n",
    "pixelwise_kwargs = {\n",
    "    'dilation_radius': dilation_radius,\n",
    "    'separate_edge_classes': separate_edge_classes,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(530, 530, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 01:52:30.384868: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2021-08-24 01:52:30.389676: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-24 01:52:30.414556: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1999995000 Hz\n",
      "2021-08-24 01:52:30.418777: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55974cd8a150 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-08-24 01:52:30.418802: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-08-24 01:52:30.420999: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "from deepcell import model_zoo\n",
    "\n",
    "#fgbg_model = model_zoo.bn_feature_net_skip_2D(\n",
    "#    n_features=2,  # segmentation mask (is_cell, is_not_cell)\n",
    "#    receptive_field=receptive_field,\n",
    "#    norm_method=norm_method,\n",
    "#    n_skips=n_skips,\n",
    "#    n_conv_filters=32,\n",
    "#    n_dense_filters=128,\n",
    "#    input_shape=tuple(X_train.shape[1:]),\n",
    "#    last_only=False)\n",
    "\n",
    "from deepcell import model_zoo\n",
    "\n",
    "input_shape=(530, 530, 1)\n",
    "\n",
    "fgbg_model = model_zoo.bn_feature_net_skip_2D(\n",
    "    n_features=2,  # segmentation mask (is_cell, is_not_cell)\n",
    "    receptive_field=receptive_field,\n",
    "    norm_method=norm_method,\n",
    "    n_skips=n_skips,\n",
    "    n_conv_filters=32,\n",
    "    n_dense_filters=128,\n",
    "    input_shape=input_shape,\n",
    "    last_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for training\n",
    "\n",
    "### Set up training parameters.\n",
    "\n",
    "There are a number of tunable hyper parameters necessary for training deep learning models:\n",
    "\n",
    "**model_name**: Incorporated into any files generated during the training process.\n",
    "\n",
    "**n_epoch**: The number of complete passes through the training dataset.\n",
    "\n",
    "**lr**: The learning rate determines the speed at which the model learns. Specifically it controls the relative size of the updates to model values after each batch.\n",
    "\n",
    "**optimizer**: The TensorFlow module [tf.keras.optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) offers optimizers with a variety of algorithm implementations. DeepCell typically uses the Adam or the SGD optimizers.\n",
    "\n",
    "**lr_sched**: A learning rate scheduler allows the learning rate to adapt over the course of model training. Typically a larger learning rate is preferred during the start of the training process, while a small learning rate allows for fine-tuning during the end of training.\n",
    "\n",
    "**batch_size**: The batch size determines the number of samples that are processed before the model is updated. The value must be greater than one and less than or equal to the number of samples in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "fgbg_model_name = 'conv_fgbg_model'\n",
    "pixelwise_model_name = 'conv_edgeseg_model'\n",
    "\n",
    "n_epoch = 5  # Number of training epochs\n",
    "\n",
    "lr = 0.01\n",
    "fgbg_optimizer = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "pixelwise_optimizer = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "lr_sched = rate_scheduler(lr=lr, decay=0.99)\n",
    "\n",
    "batch_size = 1  # fully convolutional training uses 1 image per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the DataGenerators\n",
    "\n",
    "The `ImageFullyConvDataGenerator` outputs a raw image (`X`) with it's labeled annotation mask (`y`). Additionally, it can apply a transform to `y` to change the task the model learns. Below we generate 2 training and validation data sets for both the foreground/background model and the pixelwise model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.image_generators import ImageFullyConvDataGenerator\n",
    "\n",
    "test_size = 0.2\n",
    "seed = 0\n",
    "\n",
    "#datagen = ImageFullyConvDataGenerator(\n",
    "#    rotation_range=180,\n",
    "#    zoom_range=(.8, 1.2),\n",
    "#    horizontal_flip=True,\n",
    "#    vertical_flip=True)\n",
    "\n",
    "datagen = ImageFullyConvDataGenerator(\n",
    "    rotation_range=30,\n",
    "    zoom_range=(.9, 1.1),\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False)\n",
    "\n",
    "datagen_val = ImageFullyConvDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the foreground/background data iterators\n",
    "\n",
    "fgbg_train_data = datagen.flow(\n",
    "    {'X': X_train, 'y': y_train},\n",
    "    seed=seed,\n",
    "    skip=n_skips,\n",
    "    transform='fgbg',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "fgbg_val_data = datagen_val.flow(\n",
    "    {'X': X_test, 'y': y_test},\n",
    "    seed=seed,\n",
    "    skip=n_skips,\n",
    "    transform='fgbg',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "# Skipping this step if fgbg trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pixelwise data iterators\n",
    "\n",
    "pixelwise_train_data = datagen.flow(\n",
    "    {'X': X_train, 'y': y_train},\n",
    "    seed=seed,\n",
    "    skip=n_skips,\n",
    "    transform='pixelwise',\n",
    "    transform_kwargs=pixelwise_kwargs,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "pixelwise_val_data = datagen_val.flow(\n",
    "    {'X': X_test, 'y': y_test},\n",
    "    seed=seed,\n",
    "    skip=n_skips,\n",
    "    transform='pixelwise',\n",
    "    transform_kwargs=pixelwise_kwargs,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data generator output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fgbg_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8881/3218640240.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Different data generators but same data and same seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgbg_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfgbg_train_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixelwise_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixelwise_train_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fgbg_train_data' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Different data generators but same data and same seed\n",
    "img, fgbg_output = fgbg_train_data.next()\n",
    "_, pixelwise_output = pixelwise_train_data.next()\n",
    "\n",
    "if n_skips:\n",
    "    fgbg_output = fgbg_output[0]\n",
    "    pixelwise_output = pixelwise_output[0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 15))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(img[0, ..., 0])\n",
    "ax[0].set_title('Source Image')\n",
    "\n",
    "ax[1].imshow(np.argmax(fgbg_output[0, ...], axis=-1))\n",
    "ax[1].set_title('Foreground/Background')\n",
    "\n",
    "ax[2].imshow(np.argmax(pixelwise_output[0, ...], axis=-1))\n",
    "ax[2].set_title('Pixelwise')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model with a loss function\n",
    "\n",
    "Each model is trained with it's own loss function. `weighted_categorical_crossentropy` is often used for classification models, but `weighted_focal_loss` is also supported. The losses are passed to `model.compile` before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell import losses\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    return losses.weighted_categorical_crossentropy(\n",
    "        y_true, y_pred,\n",
    "        n_classes=2,\n",
    "        from_logits=False)\n",
    "\n",
    "fgbg_model.compile(\n",
    "    loss=loss_function,\n",
    "    optimizer=fgbg_optimizer,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the foreground/background model\n",
    "\n",
    "Call `fit()` on the compiled model, along with a default set of callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 0 GPUs.\n",
      "Epoch 1/3\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.6133 - functional_5_loss: 0.3008 - functional_7_loss: 0.2945 - functional_5_accuracy: 0.8260 - functional_7_accuracy: 0.8308 \n",
      "Epoch 00001: val_loss improved from inf to 1.19154, saving model to /home/jupyter/seg_training/attempt_1/models/test/conv_fgbg_model.h5\n",
      "140/140 [==============================] - 1855s 13s/step - loss: 0.6133 - functional_5_loss: 0.3008 - functional_7_loss: 0.2945 - functional_5_accuracy: 0.8260 - functional_7_accuracy: 0.8308 - val_loss: 1.1915 - val_functional_5_loss: 0.5381 - val_functional_7_loss: 0.6353 - val_functional_5_accuracy: 0.6369 - val_functional_7_accuracy: 0.5986\n",
      "Epoch 2/3\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.5430 - functional_5_loss: 0.2666 - functional_7_loss: 0.2583 - functional_5_accuracy: 0.8365 - functional_7_accuracy: 0.8467 \n",
      "Epoch 00002: val_loss improved from 1.19154 to 0.76091, saving model to /home/jupyter/seg_training/attempt_1/models/test/conv_fgbg_model.h5\n",
      "140/140 [==============================] - 1844s 13s/step - loss: 0.5430 - functional_5_loss: 0.2666 - functional_7_loss: 0.2583 - functional_5_accuracy: 0.8365 - functional_7_accuracy: 0.8467 - val_loss: 0.7609 - val_functional_5_loss: 0.3599 - val_functional_7_loss: 0.3829 - val_functional_5_accuracy: 0.6920 - val_functional_7_accuracy: 0.6967\n",
      "Epoch 3/3\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.5427 - functional_5_loss: 0.2620 - functional_7_loss: 0.2626 - functional_5_accuracy: 0.8517 - functional_7_accuracy: 0.8493 \n",
      "Epoch 00003: val_loss did not improve from 0.76091\n",
      "140/140 [==============================] - 1839s 13s/step - loss: 0.5427 - functional_5_loss: 0.2620 - functional_7_loss: 0.2626 - functional_5_accuracy: 0.8517 - functional_7_accuracy: 0.8493 - val_loss: 0.8339 - val_functional_5_loss: 0.4102 - val_functional_7_loss: 0.4056 - val_functional_5_accuracy: 0.7187 - val_functional_7_accuracy: 0.7204\n"
     ]
    }
   ],
   "source": [
    "from deepcell.utils.train_utils import get_callbacks\n",
    "from deepcell.utils.train_utils import count_gpus\n",
    "\n",
    "model_path = os.path.join(MODEL_DIR, '{}.h5'.format(fgbg_model_name))\n",
    "loss_path = os.path.join(MODEL_DIR, '{}.npz'.format(fgbg_model_name))\n",
    "\n",
    "num_gpus = count_gpus()\n",
    "\n",
    "print('Training on', num_gpus, 'GPUs.')\n",
    "\n",
    "train_callbacks = get_callbacks(\n",
    "    model_path,\n",
    "    lr_sched=lr_sched,\n",
    "    save_weights_only=num_gpus >= 2,\n",
    "    monitor='val_loss',\n",
    "    verbose=1)\n",
    "\n",
    "loss_history = fgbg_model.fit(  # Seems like fgbg is overfitting - removed all features! \n",
    "    fgbg_train_data,\n",
    "    steps_per_epoch=fgbg_train_data.y.shape[0] // batch_size,\n",
    "    epochs=3,\n",
    "    validation_data=fgbg_val_data,\n",
    "    validation_steps=fgbg_val_data.y.shape[0] // batch_size,\n",
    "    callbacks=train_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgbg_model.save(\"../fgbg_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the `pixelwise` FeatureNet Model\n",
    "\n",
    "Here we instantiate two `FeatureNet` models from `deepcell.model_zoo` for foreground/background separation as well as the interior/edge segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fgbg_model\n",
    "fgbg_model.load_weights(\"../seg_training/attempt_1/models/test/conv_fgbg_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 530, 530, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelwise_model = model_zoo.bn_feature_net_skip_2D(\n",
    "    fgbg_model=fgbg_model,\n",
    "    n_features=4 if separate_edge_classes else 3,\n",
    "    receptive_field=receptive_field,\n",
    "    norm_method=norm_method,\n",
    "    n_skips=n_skips,\n",
    "    n_conv_filters=32,\n",
    "    n_dense_filters=128,\n",
    "    last_only=False,\n",
    "    input_shape=tuple(X_train.shape[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model with a loss function\n",
    "\n",
    "Just like the foreground/background model, the `pixelwise` model is compiled with the `weighted_categorical_crossentropy` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell import losses\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    return losses.weighted_categorical_crossentropy(\n",
    "        y_true, y_pred,\n",
    "        n_classes=4 if separate_edge_classes else 3,\n",
    "        from_logits=False)\n",
    "\n",
    "pixelwise_model.compile(\n",
    "    loss=loss_function,\n",
    "    optimizer=pixelwise_optimizer,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the `pixelwise` model\n",
    "\n",
    "Call `fit()` on the compiled model, along with a default set of callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 0 GPUs.\n",
      "Epoch 1/5\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.9915 - functional_7_loss: 0.4765 - functional_9_loss: 0.4787 - functional_7_accuracy: 0.7808 - functional_9_accuracy: 0.7856 \n",
      "Epoch 00001: val_loss improved from inf to 1.13870, saving model to /home/jupyter/seg_training/attempt_2/models/test/conv_edgeseg_model.h5\n",
      "140/140 [==============================] - 2926s 21s/step - loss: 0.9915 - functional_7_loss: 0.4765 - functional_9_loss: 0.4787 - functional_7_accuracy: 0.7808 - functional_9_accuracy: 0.7856 - val_loss: 1.1387 - val_functional_7_loss: 0.5540 - val_functional_9_loss: 0.5485 - val_functional_7_accuracy: 0.6294 - val_functional_9_accuracy: 0.6293\n",
      "Epoch 2/5\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.8073 - functional_7_loss: 0.3867 - functional_9_loss: 0.3844 - functional_7_accuracy: 0.8414 - functional_9_accuracy: 0.8406 \n",
      "Epoch 00002: val_loss improved from 1.13870 to 1.07816, saving model to /home/jupyter/seg_training/attempt_2/models/test/conv_edgeseg_model.h5\n",
      "140/140 [==============================] - 2901s 21s/step - loss: 0.8073 - functional_7_loss: 0.3867 - functional_9_loss: 0.3844 - functional_7_accuracy: 0.8414 - functional_9_accuracy: 0.8406 - val_loss: 1.0782 - val_functional_7_loss: 0.5179 - val_functional_9_loss: 0.5240 - val_functional_7_accuracy: 0.7105 - val_functional_9_accuracy: 0.7055\n",
      "Epoch 3/5\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.7571 - functional_7_loss: 0.3607 - functional_9_loss: 0.3601 - functional_7_accuracy: 0.8437 - functional_9_accuracy: 0.8438 \n",
      "Epoch 00003: val_loss did not improve from 1.07816\n",
      "140/140 [==============================] - 2906s 21s/step - loss: 0.7571 - functional_7_loss: 0.3607 - functional_9_loss: 0.3601 - functional_7_accuracy: 0.8437 - functional_9_accuracy: 0.8438 - val_loss: 1.1360 - val_functional_7_loss: 0.5461 - val_functional_9_loss: 0.5537 - val_functional_7_accuracy: 0.6828 - val_functional_9_accuracy: 0.6930\n",
      "Epoch 4/5\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.7247 - functional_7_loss: 0.3478 - functional_9_loss: 0.3407 - functional_7_accuracy: 0.8465 - functional_9_accuracy: 0.8494 \n",
      "Epoch 00004: val_loss improved from 1.07816 to 1.01267, saving model to /home/jupyter/seg_training/attempt_2/models/test/conv_edgeseg_model.h5\n",
      "140/140 [==============================] - 2894s 21s/step - loss: 0.7247 - functional_7_loss: 0.3478 - functional_9_loss: 0.3407 - functional_7_accuracy: 0.8465 - functional_9_accuracy: 0.8494 - val_loss: 1.0127 - val_functional_7_loss: 0.4905 - val_functional_9_loss: 0.4859 - val_functional_7_accuracy: 0.7076 - val_functional_9_accuracy: 0.7105\n",
      "Epoch 5/5\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.7286 - functional_7_loss: 0.3456 - functional_9_loss: 0.3467 - functional_7_accuracy: 0.8440 - functional_9_accuracy: 0.8455 \n",
      "Epoch 00005: val_loss did not improve from 1.01267\n",
      "140/140 [==============================] - 2901s 21s/step - loss: 0.7286 - functional_7_loss: 0.3456 - functional_9_loss: 0.3467 - functional_7_accuracy: 0.8440 - functional_9_accuracy: 0.8455 - val_loss: 1.0947 - val_functional_7_loss: 0.5247 - val_functional_9_loss: 0.5337 - val_functional_7_accuracy: 0.7036 - val_functional_9_accuracy: 0.6934\n"
     ]
    }
   ],
   "source": [
    "from deepcell.utils.train_utils import get_callbacks\n",
    "from deepcell.utils.train_utils import count_gpus\n",
    "\n",
    "model_path = os.path.join(MODEL_DIR, '{}.h5'.format(pixelwise_model_name))\n",
    "loss_path = os.path.join(MODEL_DIR, '{}.npz'.format(pixelwise_model_name))\n",
    "\n",
    "num_gpus = count_gpus()\n",
    "\n",
    "print('Training on', num_gpus, 'GPUs.')\n",
    "\n",
    "train_callbacks = get_callbacks(\n",
    "    model_path,\n",
    "    lr_sched=lr_sched,\n",
    "    save_weights_only=num_gpus >= 2,\n",
    "    monitor='val_loss',\n",
    "    verbose=1)\n",
    "\n",
    "loss_history = pixelwise_model.fit(\n",
    "    pixelwise_train_data,\n",
    "    steps_per_epoch=pixelwise_train_data.y.shape[0] // batch_size,\n",
    "    epochs=5,\n",
    "    validation_data=pixelwise_val_data,\n",
    "    validation_steps=pixelwise_val_data.y.shape[0] // batch_size,\n",
    "    callbacks=train_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelwise_model.save(\"../pixelwise_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test data\n",
    "\n",
    "Use the trained model to predict on new data and post-process the results into a label mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelwise_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = pixelwise_model.predict(X_test)[-1]\n",
    "test_images_fgbg = fgbg_model.predict(X_test)[-1]\n",
    "\n",
    "print('watershed transform shape:', test_images.shape)\n",
    "print('segmentation mask shape:', test_images_fgbg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold the foreground/background\n",
    "# and remove back ground from edge transform\n",
    "threshold = 0.8\n",
    "\n",
    "fg_thresh = test_images_fgbg[..., 1] > threshold\n",
    "fg_thresh = np.expand_dims(fg_thresh, axis=-1)\n",
    "\n",
    "test_images_post_fgbg = test_images * fg_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label interior predictions\n",
    "from skimage.measure import label\n",
    "from skimage import morphology\n",
    "\n",
    "labeled_images = []\n",
    "for i in range(test_images_post_fgbg.shape[0]):\n",
    "    interior = test_images_post_fgbg[i, ..., 2] > .5\n",
    "    labeled_image = label(interior)\n",
    "    labeled_image = morphology.remove_small_objects(\n",
    "        labeled_image, min_size=130, connectivity=1)  ## min_size=50\n",
    "    labeled_images.append(labeled_image)\n",
    "labeled_images = np.array(labeled_images)\n",
    "labeled_images = np.expand_dims(labeled_images, axis=-1)  \n",
    "\n",
    "print('labeled_images shape:', labeled_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#index = np.random.randint(low=0, high=X_test.shape[0])\n",
    "\n",
    "index = 3\n",
    "print('Image number:', index)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(15, 15), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(X_test[index, ..., 0])\n",
    "ax[0].set_title('Source Image')\n",
    "\n",
    "ax[1].imshow(test_images_fgbg[index, ..., 1])\n",
    "ax[1].set_title('Segmentation Prediction')\n",
    "\n",
    "ax[2].imshow(fg_thresh[index, ..., 0], cmap='jet')\n",
    "ax[2].set_title('FGBG Threshold {}%'.format(threshold * 100))\n",
    "\n",
    "ax[3].imshow(test_images[index, ..., 0] + test_images[index, ..., 1], cmap='jet')\n",
    "ax[3].set_title('Edge Prediction')\n",
    "\n",
    "ax[4].imshow(test_images[index, ..., 2], cmap='jet')\n",
    "ax[4].set_title('Interior Prediction')\n",
    "\n",
    "ax[5].imshow(labeled_images[index, ..., 0], cmap='jet')\n",
    "ax[5].set_title('Instance Segmentation')\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(labeled_images[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILE = \"../validation.trks\"\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data('../validation.npz', mode='sample', test_size=1)\n",
    "print('X.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y.shape: {}\\ny.shape: {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to stack them together\n",
    "## Store them into seg_validation.npz\n",
    "\n",
    "val_all = np.load('../validation.npz')\n",
    "X = val_all['X']\n",
    "y = val_all['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m78"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
